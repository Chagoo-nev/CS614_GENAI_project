"""
Main entry point for the CS614 GenAI project.
Controls the workflow for model training, quantization, and evaluation.
Optimized for Google Colab environment.
"""

import os
import sys
import argparse
import torch
import re
import time
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Import project utilities
from utils.drive_utils import mount_drive, load_model_from_drive, save_model_to_drive
from utils.data_utils import load_gsm8k_data, get_fewshot_examples, create_fewshot_prompt, extract_reference_answer
from utils.eval_utils import calculate_metrics, save_results, print_evaluation_summary

# For simplified evaluation without checker model
def simplified_check_answer(generated_solution, reference_answer):
    """
    Directly compare the numeric value after `####` or similar pattern.
    
    Args:
        generated_solution: Answer generated by the model
        reference_answer: Correct answer from the GSM8K dataset
    
    Returns:
        bool: Whether the answers match
    """
    # Extract the reference answer number
    ref_match = re.search(r'####\s*(-?[\d.]+)', reference_answer)
    if not ref_match:
        return False
    ref_answer = ref_match.group(1)
    
    # Extract the model-generated answer number
    # Look for patterns like "final answer is 42" or "#### 42"
    gen_patterns = [
        r'####\s*(-?[\d.]+)',  # Standard GSM8K format
        r'(?:final answer|answer|result)(?:\s+is)?[:\s]+(-?[\d.]+)',  # Various text patterns
        r'[-\d.]+$'  # Last number in text
    ]
    
    for pattern in gen_patterns:
        gen_match = re.search(pattern, generated_solution, re.IGNORECASE)
        if gen_match:
            gen_answer = gen_match.group(1)
            return gen_answer.strip() == ref_answer.strip()
    
    # If no pattern matched, try to find any numbers in the text
    numbers = re.findall(r'-?[\d.]+', generated_solution)
    if numbers:
        gen_answer = numbers[-1]  # Take the last number found
        return gen_answer.strip() == ref_answer.strip()
    
    return False

def generate_solution(model, tokenizer, prompt, max_new_tokens=512, temperature=0.0):
    """
    Generate a solution for a math problem using the model.
    
    Args:
        model: The language model
        tokenizer: The tokenizer
        prompt: Input prompt
        max_new_tokens: Maximum number of tokens to generate
        temperature: Sampling temperature (0 for deterministic)
        
    Returns:
        tuple: (solution, inference_time)
    """
    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Record inference time
    start_time = time.time()

    # Generate answer
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=False if temperature == 0 else True,
            repetition_penalty=1.2
        )

    inference_time = time.time() - start_time

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract just the generated part (remove the prompt)
    solution = generated_text[len(prompt):]

    return solution, inference_time

# Function definitions
def run_lora_training(model_name="meta-llama/Llama-3.1-8B", output_dir="./lora_output", 
                      num_epochs=3, batch_size=4, learning_rate=5e-5, max_length=512,
                      lora_r=8, lora_alpha=16, lora_dropout=0.05, seed=42):
    """
    Run LoRA training directly in the notebook instead of spawning a subprocess.
    This function integrates the LoRA training logic for better Colab compatibility.
    
    Args:
        model_name: Base model to fine-tune
        output_dir: Directory to save outputs
        num_epochs: Number of training epochs
        batch_size: Training batch size
        learning_rate: Learning rate
        max_length: Maximum sequence length
        lora_r: LoRA attention dimension
        lora_alpha: LoRA alpha parameter
        lora_dropout: LoRA dropout rate
        seed: Random seed
        
    Returns:
        str: Path to the saved model
    """
    from peft import LoraConfig, get_peft_model, TaskType
    from transformers import Trainer, TrainingArguments, default_data_collator
    from utils.data_utils import set_seed, load_gsm8k_data
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Set seed for reproducibility
    set_seed(seed)
    
    # Load GSM8K dataset
    print("Loading GSM8K dataset...")
    dataset = load_gsm8k_data()
    train_dataset = dataset["train"]
    
    # Load model and tokenizer
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Handle special tokens
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load model with reduced precision for efficiency
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # Preprocess dataset (define inside the function for better encapsulation)
    def preprocess_function(examples):
        prompts = []
        for question, answer in zip(examples["question"], examples["answer"]):
            prompt = f"Question: {question}\nAnswer: {answer}"
            prompts.append(prompt)
        
        tokenized = tokenizer(
            prompts,
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
        
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    print("Preprocessing data...")
    tokenized_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=["question", "answer"]
    )
    
    # LoRA configuration
    print("Setting up LoRA configuration...")
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # Adapt as needed for your model
    )
    
    # Create PEFT model
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        save_strategy="epoch",
        learning_rate=learning_rate,
        fp16=True,
        report_to="tensorboard",
    )
    
    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=default_data_collator,
        tokenizer=tokenizer,
    )
    
    # Train the model
    print("Starting LoRA fine-tuning...")
    trainer.train()
    
    # Save fine-tuned model
    final_model_dir = os.path.join(output_dir, "final_model")
    trainer.save_model(final_model_dir)
    print(f"Model saved to {final_model_dir}")
    
    return final_model_dir

def run_quantization(model_path, save_path, bits=8, method="dynamic"):
    """
    Apply Post-Training Quantization (PTQ) to a model.
    
    Args:
        model_path: Path to the model to quantize
        save_path: Path to save the quantized model
        bits: Quantization precision (4 or 8)
        method: Quantization method ("dynamic" or "static")
        
    Returns:
        Path to the quantized model
    """
    import torch
    from torch.ao.quantization import quantize_dynamic, default_dynamic_qconfig
    
    print(f"Starting Post-Training Quantization ({bits}-bit, {method})...")
    
    # Create save directory
    os.makedirs(save_path, exist_ok=True)
    
    # Load model
    if "lora" in model_path.lower():
        # If it's a LoRA model, load the base model first, then apply LoRA weights
        config = PeftConfig.from_pretrained(model_path)
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path, 
            torch_dtype=torch.float16,
            device_map="auto"
        )
        model = PeftModel.from_pretrained(base_model, model_path)
        # Merge LoRA weights for quantization
        model = model.merge_and_unload()
    else:
        # Load regular model
        model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
    
    # Move to CPU for quantization
    model = model.to("cpu")
    
    if bits == 8:
        if method == "dynamic":
            # Dynamic quantization - only weights are quantized, activations dynamically
            print("Applying 8-bit dynamic quantization...")
            quantized_model = torch.ao.quantization.quantize_dynamic(
                model, {torch.nn.Linear}, dtype=torch.qint8
            )
        else:
            # Static quantization would require calibration data
            print("Static 8-bit quantization not yet implemented")
            return None
    elif bits == 4:
        # 4-bit quantization using optimum or bitsandbytes if available
        try:
            # Try using optimum for 4-bit quantization
            from optimum.bettertransformer import BetterTransformer
            model = BetterTransformer.transform(model)
            print("Applied BetterTransformer optimizations")
            
            # Note: Full 4-bit quantization would require additional libraries
            print("Warning: Full 4-bit quantization requires additional steps")
            quantized_model = model
        except ImportError:
            print("4-bit quantization requires optimum or bitsandbytes library")
            return None
    
    # Save the quantized model
    save_path_with_bits = f"{save_path}_{bits}bit"
    os.makedirs(save_path_with_bits, exist_ok=True)
    
    try:
        quantized_model.save_pretrained(save_path_with_bits)
        print(f"Quantized model saved to {save_path_with_bits}")
        
        # Also save the tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        tokenizer.save_pretrained(save_path_with_bits)
        
        # Save quantization metadata
        with open(f"{save_path_with_bits}/quantization_info.json", "w") as f:
            json.dump({
                "original_model": model_path,
                "bits": bits,
                "method": method,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }, f, indent=2)
            
        return save_path_with_bits
    except Exception as e:
        print(f"Error saving quantized model: {e}")
        return None

def run_evaluation(model, tokenizer, num_samples=100, max_new_tokens=512, 
                  fewshot=5, seed=42, save_dir="./results", verbose=True):
    """
    Evaluate a model on the GSM8K benchmark using few-shot prompting.
    Simplified version that doesn't require a checker model.
    
    Args:
        model: The model to evaluate
        tokenizer: The tokenizer
        num_samples: Number of samples to evaluate
        max_new_tokens: Maximum tokens to generate
        fewshot: Number of few-shot examples
        seed: Random seed
        save_dir: Directory to save results
        verbose: Whether to print detailed progress
        
    Returns:
        dict: Evaluation results
    """
    # Load dataset
    dataset = load_gsm8k_data()
    
    # Extract few-shot examples
    fewshot_examples = get_fewshot_examples(dataset, num_examples=fewshot, seed=seed)
    print(f"Extracted {len(fewshot_examples)} few-shot examples with seed {seed}")
    
    # Select test samples
    if num_samples < len(dataset["test"]):
        test_subset = dataset["test"].select(range(num_samples))
    else:
        test_subset = dataset["test"]
        num_samples = len(test_subset)
    
    print(f"Evaluating on {num_samples} samples from GSM8K test set")
    
    # Track metrics
    correct = 0
    latencies = []
    example_results = []
    
    # Set model to evaluation mode
    model.eval()
    
    # Start evaluation timer
    start_time = time.time()
    
    # Process each test example
    for i, example in enumerate(test_subset):
        if verbose:
            print(f"Processing example {i+1}/{num_samples}...")
        
        question = example["question"]
        reference_answer = example["answer"]
        
        # Create few-shot prompt
        prompt = create_fewshot_prompt(question, fewshot_examples)
        
        # Generate solution
        solution, inference_time = generate_solution(
            model, 
            tokenizer, 
            prompt, 
            max_new_tokens=max_new_tokens
        )
        latencies.append(inference_time)
        
        # Check if answer is correct
        is_correct = simplified_check_answer(solution, reference_answer)
        if is_correct:
            correct += 1
        
        # Store result
        example_results.append({
            "question": question,
            "reference_answer": reference_answer,
            "generated_solution": solution,
            "is_correct": is_correct,
            "inference_time": inference_time
        })
        
        # Print progress
        if verbose and (i+1) % 5 == 0:
            print(f"Progress: {i+1}/{num_samples} examples processed")
            print(f"Current accuracy: {correct/(i+1)*100:.2f}%")
            print(f"Average inference time: {sum(latencies)/(i+1):.2f}s")
        
    # Calculate metrics
    results = calculate_metrics(correct, num_samples, latencies, start_time)
    
    # Print summary
    print_evaluation_summary(results)
    
    # Save results
    save_results(results, example_results, save_dir)
    
    return results

def run_colab_workflow(mode='train', model_name="meta-llama/Llama-3.1-8B", 
                     lora_output_dir="./lora_output", quant_bits=8, eval_samples=50, 
                     from_drive=False, to_drive=False):
    """
    Run the complete workflow in a Colab-friendly way.
    
    Args:
        mode: Operation mode ('train', 'evaluate', 'quantize', 'all')
        model_name: Base model name
        lora_output_dir: Output directory for LoRA training
        quant_bits: Quantization precision
        eval_samples: Number of evaluation samples
        from_drive: Whether to load models from Google Drive
        to_drive: Whether to save models to Google Drive
        
    Returns:
        dict: Results for each operation
    """
    results = {}
    
    # Mount Google Drive if needed
    if from_drive or to_drive:
        mount_drive()
    
    # --- TRAINING ---
    if mode == 'train' or mode == 'all':
        print("\n=== TRAINING MODE ===")
        
        # Run LoRA training
        lora_model_path = run_lora_training(
            model_name=model_name,
            output_dir=lora_output_dir
        )
        
        # Save to Drive if requested
        if to_drive and lora_model_path:
            drive_path = "/content/drive/MyDrive/models/lora_gsm8k"
            print(f"Saving LoRA model to Google Drive: {drive_path}")
            
            # Implement saving logic here
            # This could be a simple file copy or using special save functions
            
        results['training'] = {
            'lora_model_path': lora_model_path,
            'completed': lora_model_path is not None
        }
    
    # --- QUANTIZATION ---
    if mode == 'quantize' or mode == 'all':
        print("\n=== QUANTIZATION MODE ===")
        
        # Determine which model to quantize
        if mode == 'all' and 'training' in results and results['training']['completed']:
            # Use the just-trained LoRA model
            model_to_quantize = results['training']['lora_model_path']
        else:
            # Use a specified model
            if from_drive:
                # Logic to load from Drive
                model_to_quantize = "/content/drive/MyDrive/models/lora_gsm8k"
            else:
                model_to_quantize = lora_output_dir + "/final_model"
        
        # Apply quantization
        quantized_model_path = run_quantization(
            model_path=model_to_quantize,
            save_path="./quantized_model",
            bits=quant_bits
        )
        
        results['quantization'] = {
            'quantized_model_path': quantized_model_path,
            'bits': quant_bits,
            'completed': quantized_model_path is not None
        }
    
    # --- EVALUATION ---
    if mode == 'evaluate' or mode == 'all':
        print("\n=== EVALUATION MODE ===")
        
        # Determine which model to evaluate
        if mode == 'all' and 'quantization' in results and results['quantization']['completed']:
            # Evaluate the just-quantized model
            model_path = results['quantization']['quantized_model_path']
        elif mode == 'all' and 'training' in results and results['training']['completed']:
            # Evaluate the just-trained LoRA model
            model_path = results['training']['lora_model_path']
        else:
            # Evaluate a specified model
            if from_drive:
                model_path = "/content/drive/MyDrive/models/Llama-3.1-8B"  # Example
            else:
                model_path = model_name  # Use the original model
        
        # Load model for evaluation
        print(f"Loading model for evaluation from: {model_path}")
        try:
            if from_drive and "drive/MyDrive" in model_path:
                # Use special loading function for Drive models
                model, tokenizer = load_model_from_drive(os.path.basename(model_path))
            else:
                # Regular loading
                if "lora" in model_path.lower():
                    # Load a LoRA model
                    config = PeftConfig.from_pretrained(model_path)
                    base_model = AutoModelForCausalLM.from_pretrained(
                        config.base_model_name_or_path,
                        torch_dtype=torch.float16,
                        device_map="auto"
                    )
                    model = PeftModel.from_pretrained(base_model, model_path)
                    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
                else:
                    # Load a regular model
                    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Run evaluation
            if model is not None and tokenizer is not None:
                eval_results = run_evaluation(
                    model=model, 
                    tokenizer=tokenizer,
                    num_samples=eval_samples,
                    save_dir="./evaluation_results"
                )
                results['evaluation'] = eval_results
            else:
                print("Failed to load model for evaluation")
                results['evaluation'] = {"completed": False, "error": "Model loading failed"}
        except Exception as e:
            print(f"Error during evaluation: {e}")
            results['evaluation'] = {"completed": False, "error": str(e)}
    
    print("\n=== WORKFLOW COMPLETED ===")
    return results

# Main entry point compatible with both command-line usage and direct Colab calls
def main():
    """
    Main function to control the workflow.
    Can be called from command line or directly in a notebook.
    """
    # If called with arguments, parse them
    if len(sys.argv) > 1:
        parser = argparse.ArgumentParser(description="CS614 GenAI Project - GSM8K Evaluation")
        
        # Mode selection
        parser.add_argument("--mode", type=str, default="all", 
                          choices=['train', 'evaluate', 'quantize', 'all'],
                          help="Operating mode: train (LoRA), evaluate, quantize (PTQ), or all")
        
        # Model configuration
        parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.1-8B", 
                            help="Base model name")
        
        # LoRA configuration
        parser.add_argument("--lora_output", type=str, default="./lora_output", 
                            help="Output directory for LoRA training")
        
        # Quantization configuration
        parser.add_argument("--quant_bits", type=int, default=8, choices=[4, 8], 
                            help="Quantization precision (4 or 8 bits)")
        
        # Evaluation configuration
        parser.add_argument("--eval_samples", type=int, default=50, 
                            help="Number of samples to evaluate")
        
        # Drive configuration
        parser.add_argument("--from_drive", action="store_true", 
                            help="Load models from Google Drive")
        parser.add_argument("--to_drive", action="store_true", 
                            help="Save models to Google Drive")
        
        args = parser.parse_args()
        
        # Run the workflow with parsed arguments
        return run_colab_workflow(
            mode=args.mode,
            model_name=args.model_name,
            lora_output_dir=args.lora_output,
            quant_bits=args.quant_bits,
            eval_samples=args.eval_samples,
            from_drive=args.from_drive,
            to_drive=args.to_drive
        )
    else:
        # If called without arguments (e.g., in a notebook),
        # just return the function for manual calling
        return run_colab_workflow

if __name__ == "__main__":
    main()